{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stock symbols from CSV\n",
    "csv_file = \"stock_symbol.csv\"\n",
    "symbols_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Validate CSV data\n",
    "if 'Symbol' not in symbols_df.columns:\n",
    "    raise ValueError(\"CSV file must contain a 'Symbol' column.\")\n",
    "\n",
    "stock_symbols = symbols_df['Symbol'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"3\" halign=\"left\">High</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>...</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.440331</td>\n",
       "      <td>6.695000</td>\n",
       "      <td>15.536651</td>\n",
       "      <td>23.254053</td>\n",
       "      <td>7.640000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.455077</td>\n",
       "      <td>6.830500</td>\n",
       "      <td>15.605068</td>\n",
       "      <td>...</td>\n",
       "      <td>15.541608</td>\n",
       "      <td>23.006110</td>\n",
       "      <td>7.931429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493729600</td>\n",
       "      <td>151998000</td>\n",
       "      <td>78541293</td>\n",
       "      <td>38409100</td>\n",
       "      <td>17239600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>6.451465</td>\n",
       "      <td>6.734500</td>\n",
       "      <td>15.468233</td>\n",
       "      <td>23.261560</td>\n",
       "      <td>7.358571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.487878</td>\n",
       "      <td>6.774000</td>\n",
       "      <td>15.563671</td>\n",
       "      <td>...</td>\n",
       "      <td>15.547310</td>\n",
       "      <td>23.178914</td>\n",
       "      <td>7.652857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601904800</td>\n",
       "      <td>177038000</td>\n",
       "      <td>120638494</td>\n",
       "      <td>49749600</td>\n",
       "      <td>23753100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>6.348845</td>\n",
       "      <td>6.612500</td>\n",
       "      <td>15.078298</td>\n",
       "      <td>23.118807</td>\n",
       "      <td>7.617143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.477044</td>\n",
       "      <td>6.736500</td>\n",
       "      <td>15.514588</td>\n",
       "      <td>...</td>\n",
       "      <td>15.514588</td>\n",
       "      <td>23.201454</td>\n",
       "      <td>7.361429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>552160000</td>\n",
       "      <td>143576000</td>\n",
       "      <td>159744526</td>\n",
       "      <td>58182400</td>\n",
       "      <td>23290400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>6.337111</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>14.727283</td>\n",
       "      <td>22.878380</td>\n",
       "      <td>7.485714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.379844</td>\n",
       "      <td>6.616000</td>\n",
       "      <td>15.121432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.106558</td>\n",
       "      <td>23.013620</td>\n",
       "      <td>7.731429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>477131200</td>\n",
       "      <td>220604000</td>\n",
       "      <td>257533695</td>\n",
       "      <td>50559700</td>\n",
       "      <td>9955400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>6.379241</td>\n",
       "      <td>6.676000</td>\n",
       "      <td>14.923612</td>\n",
       "      <td>23.036160</td>\n",
       "      <td>7.614286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.379843</td>\n",
       "      <td>6.684000</td>\n",
       "      <td>14.954102</td>\n",
       "      <td>...</td>\n",
       "      <td>14.675223</td>\n",
       "      <td>22.750650</td>\n",
       "      <td>7.498571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>447610800</td>\n",
       "      <td>196610000</td>\n",
       "      <td>189680313</td>\n",
       "      <td>51197400</td>\n",
       "      <td>8180900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>188.380005</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>147.740005</td>\n",
       "      <td>359.839996</td>\n",
       "      <td>855.859985</td>\n",
       "      <td>239.429993</td>\n",
       "      <td>199.880005</td>\n",
       "      <td>178.139999</td>\n",
       "      <td>153.089996</td>\n",
       "      <td>...</td>\n",
       "      <td>149.899994</td>\n",
       "      <td>364.130005</td>\n",
       "      <td>896.500000</td>\n",
       "      <td>255.380005</td>\n",
       "      <td>125910900</td>\n",
       "      <td>123159400</td>\n",
       "      <td>39832200</td>\n",
       "      <td>49209900</td>\n",
       "      <td>6798800</td>\n",
       "      <td>181229400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>181.460007</td>\n",
       "      <td>175.259995</td>\n",
       "      <td>149.240005</td>\n",
       "      <td>357.859985</td>\n",
       "      <td>867.830017</td>\n",
       "      <td>233.289993</td>\n",
       "      <td>194.149994</td>\n",
       "      <td>183.410004</td>\n",
       "      <td>154.929993</td>\n",
       "      <td>...</td>\n",
       "      <td>143.389999</td>\n",
       "      <td>350.880005</td>\n",
       "      <td>827.849976</td>\n",
       "      <td>223.779999</td>\n",
       "      <td>160466300</td>\n",
       "      <td>109327100</td>\n",
       "      <td>47823000</td>\n",
       "      <td>50425000</td>\n",
       "      <td>6656800</td>\n",
       "      <td>183453800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839</th>\n",
       "      <td>2025-04-08</td>\n",
       "      <td>172.419998</td>\n",
       "      <td>170.660004</td>\n",
       "      <td>146.580002</td>\n",
       "      <td>354.559998</td>\n",
       "      <td>870.400024</td>\n",
       "      <td>221.860001</td>\n",
       "      <td>190.339996</td>\n",
       "      <td>185.899994</td>\n",
       "      <td>154.440002</td>\n",
       "      <td>...</td>\n",
       "      <td>153.574997</td>\n",
       "      <td>368.260010</td>\n",
       "      <td>912.440002</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>120859500</td>\n",
       "      <td>87710400</td>\n",
       "      <td>35304400</td>\n",
       "      <td>35868900</td>\n",
       "      <td>5625400</td>\n",
       "      <td>171603500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>198.850006</td>\n",
       "      <td>191.100006</td>\n",
       "      <td>161.059998</td>\n",
       "      <td>390.489990</td>\n",
       "      <td>945.469971</td>\n",
       "      <td>272.200012</td>\n",
       "      <td>200.610001</td>\n",
       "      <td>192.649994</td>\n",
       "      <td>161.869995</td>\n",
       "      <td>...</td>\n",
       "      <td>146.330002</td>\n",
       "      <td>353.540009</td>\n",
       "      <td>855.929993</td>\n",
       "      <td>224.690002</td>\n",
       "      <td>184395900</td>\n",
       "      <td>116804300</td>\n",
       "      <td>46479500</td>\n",
       "      <td>50199700</td>\n",
       "      <td>7498000</td>\n",
       "      <td>219433400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>190.419998</td>\n",
       "      <td>181.220001</td>\n",
       "      <td>155.369995</td>\n",
       "      <td>381.350006</td>\n",
       "      <td>921.169983</td>\n",
       "      <td>252.399994</td>\n",
       "      <td>194.779999</td>\n",
       "      <td>186.869995</td>\n",
       "      <td>160.029999</td>\n",
       "      <td>...</td>\n",
       "      <td>158.759995</td>\n",
       "      <td>382.059998</td>\n",
       "      <td>931.940002</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>121685000</td>\n",
       "      <td>68082500</td>\n",
       "      <td>35218900</td>\n",
       "      <td>37944100</td>\n",
       "      <td>5116800</td>\n",
       "      <td>180875500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3842 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        Date       Close                                                  \\\n",
       "Ticker                   AAPL        AMZN        GOOG        MSFT        NFLX   \n",
       "0      2010-01-04    6.440331    6.695000   15.536651   23.254053    7.640000   \n",
       "1      2010-01-05    6.451465    6.734500   15.468233   23.261560    7.358571   \n",
       "2      2010-01-06    6.348845    6.612500   15.078298   23.118807    7.617143   \n",
       "3      2010-01-07    6.337111    6.500000   14.727283   22.878380    7.485714   \n",
       "4      2010-01-08    6.379241    6.676000   14.923612   23.036160    7.614286   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "3837   2025-04-04  188.380005  171.000000  147.740005  359.839996  855.859985   \n",
       "3838   2025-04-07  181.460007  175.259995  149.240005  357.859985  867.830017   \n",
       "3839   2025-04-08  172.419998  170.660004  146.580002  354.559998  870.400024   \n",
       "3840   2025-04-09  198.850006  191.100006  161.059998  390.489990  945.469971   \n",
       "3841   2025-04-10  190.419998  181.220001  155.369995  381.350006  921.169983   \n",
       "\n",
       "Price                     High                          ...        Open  \\\n",
       "Ticker        TSLA        AAPL        AMZN        GOOG  ...        GOOG   \n",
       "0              NaN    6.455077    6.830500   15.605068  ...   15.541608   \n",
       "1              NaN    6.487878    6.774000   15.563671  ...   15.547310   \n",
       "2              NaN    6.477044    6.736500   15.514588  ...   15.514588   \n",
       "3              NaN    6.379844    6.616000   15.121432  ...   15.106558   \n",
       "4              NaN    6.379843    6.684000   14.954102  ...   14.675223   \n",
       "...            ...         ...         ...         ...  ...         ...   \n",
       "3837    239.429993  199.880005  178.139999  153.089996  ...  149.899994   \n",
       "3838    233.289993  194.149994  183.410004  154.929993  ...  143.389999   \n",
       "3839    221.860001  190.339996  185.899994  154.440002  ...  153.574997   \n",
       "3840    272.200012  200.610001  192.649994  161.869995  ...  146.330002   \n",
       "3841    252.399994  194.779999  186.869995  160.029999  ...  158.759995   \n",
       "\n",
       "Price                                          Volume                        \\\n",
       "Ticker        MSFT        NFLX        TSLA       AAPL       AMZN       GOOG   \n",
       "0        23.006110    7.931429         NaN  493729600  151998000   78541293   \n",
       "1        23.178914    7.652857         NaN  601904800  177038000  120638494   \n",
       "2        23.201454    7.361429         NaN  552160000  143576000  159744526   \n",
       "3        23.013620    7.731429         NaN  477131200  220604000  257533695   \n",
       "4        22.750650    7.498571         NaN  447610800  196610000  189680313   \n",
       "...            ...         ...         ...        ...        ...        ...   \n",
       "3837    364.130005  896.500000  255.380005  125910900  123159400   39832200   \n",
       "3838    350.880005  827.849976  223.779999  160466300  109327100   47823000   \n",
       "3839    368.260010  912.440002  245.000000  120859500   87710400   35304400   \n",
       "3840    353.540009  855.929993  224.690002  184395900  116804300   46479500   \n",
       "3841    382.059998  931.940002  260.000000  121685000   68082500   35218900   \n",
       "\n",
       "Price                                    \n",
       "Ticker      MSFT      NFLX         TSLA  \n",
       "0       38409100  17239600          NaN  \n",
       "1       49749600  23753100          NaN  \n",
       "2       58182400  23290400          NaN  \n",
       "3       50559700   9955400          NaN  \n",
       "4       51197400   8180900          NaN  \n",
       "...          ...       ...          ...  \n",
       "3837    49209900   6798800  181229400.0  \n",
       "3838    50425000   6656800  183453800.0  \n",
       "3839    35868900   5625400  171603500.0  \n",
       "3840    50199700   7498000  219433400.0  \n",
       "3841    37944100   5116800  180875500.0  \n",
       "\n",
       "[3842 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = pd.Timestamp.today()\n",
    "data = yf.download(stock_symbols, start = start_date, end = end_date)\n",
    "data.reset_index(inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stock: GOOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3842\n",
      "Training Data shape: (3073, 2)\n",
      "Testing Data shape: (769, 2)\n",
      "Processing stock: AAPL\n",
      "Total data points: 3842\n",
      "Training Data shape: (3073, 2)\n",
      "Testing Data shape: (769, 2)\n",
      "Processing stock: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3842\n",
      "Training Data shape: (3073, 2)\n",
      "Testing Data shape: (769, 2)\n",
      "Processing stock: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3842\n",
      "Training Data shape: (3073, 2)\n",
      "Testing Data shape: (769, 2)\n",
      "Processing stock: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3720\n",
      "Training Data shape: (2976, 2)\n",
      "Testing Data shape: (744, 2)\n",
      "Processing stock: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 3842\n",
      "Training Data shape: (3073, 2)\n",
      "Testing Data shape: (769, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results for each stock\n",
    "predictions_dict = {}\n",
    "\n",
    "for stock_symbol in stock_symbols:\n",
    "    print(f\"Processing stock: {stock_symbol}\")\n",
    "    data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    # Determine the cutoff point (80% for training, 20% for testing)\n",
    "    cutoff = int(len(data) * 0.8)\n",
    "\n",
    "    # Step 1: Prepare the training and testing data using iloc\n",
    "    data_train = data.iloc[:cutoff][['Open', 'Close']].copy()\n",
    "    data_test = data.iloc[cutoff:][['Open', 'Close']].copy()\n",
    "\n",
    "    # Display the shapes of both datasets\n",
    "    print(f\"Total data points: {len(data)}\")\n",
    "    print(f\"Training Data shape: {data_train.shape}\")\n",
    "    print(f\"Testing Data shape: {data_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_train_scaled = scaler.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the input (x) and output (y) for the LSTM model\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Create sequences of data\n",
    "time_steps = 60  # Number of time steps to look back\n",
    "\n",
    "for i in range(time_steps, data_train_scaled.shape[0]):\n",
    "    x.append(data_train_scaled[i - time_steps:i])  # Previous 60 time steps\n",
    "    y.append(data_train_scaled[i])  # Current Open and Close prices\n",
    "\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer to predict both Open and Close prices\n",
    "model.add(Dense(units=2))  # 2 units for Open and Close prices\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "95/95 [==============================] - 18s 155ms/step - loss: 0.0225\n",
      "Epoch 2/100\n",
      "95/95 [==============================] - 15s 158ms/step - loss: 0.0067\n",
      "Epoch 3/100\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.0054\n",
      "Epoch 4/100\n",
      "95/95 [==============================] - 14s 150ms/step - loss: 0.0054\n",
      "Epoch 5/100\n",
      "95/95 [==============================] - 14s 149ms/step - loss: 0.0050\n",
      "Epoch 6/100\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.0047\n",
      "Epoch 7/100\n",
      "95/95 [==============================] - 15s 158ms/step - loss: 0.0047\n",
      "Epoch 8/100\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.0041\n",
      "Epoch 9/100\n",
      "95/95 [==============================] - 14s 149ms/step - loss: 0.0035\n",
      "Epoch 10/100\n",
      "95/95 [==============================] - 16s 166ms/step - loss: 0.0038\n",
      "Epoch 11/100\n",
      "95/95 [==============================] - 20s 205ms/step - loss: 0.0036\n",
      "Epoch 12/100\n",
      "95/95 [==============================] - 17s 174ms/step - loss: 0.0036\n",
      "Epoch 13/100\n",
      "95/95 [==============================] - 16s 172ms/step - loss: 0.0033\n",
      "Epoch 14/100\n",
      "95/95 [==============================] - 16s 171ms/step - loss: 0.0031\n",
      "Epoch 15/100\n",
      "95/95 [==============================] - 17s 181ms/step - loss: 0.0038\n",
      "Epoch 16/100\n",
      "95/95 [==============================] - 17s 174ms/step - loss: 0.0028\n",
      "Epoch 17/100\n",
      "95/95 [==============================] - 17s 183ms/step - loss: 0.0028\n",
      "Epoch 18/100\n",
      "95/95 [==============================] - 16s 163ms/step - loss: 0.0032\n",
      "Epoch 19/100\n",
      "95/95 [==============================] - 16s 166ms/step - loss: 0.0030\n",
      "Epoch 20/100\n",
      "95/95 [==============================] - 15s 160ms/step - loss: 0.0031\n",
      "Epoch 21/100\n",
      "95/95 [==============================] - 15s 162ms/step - loss: 0.0029\n",
      "Epoch 22/100\n",
      "95/95 [==============================] - 15s 156ms/step - loss: 0.0029\n",
      "Epoch 23/100\n",
      "95/95 [==============================] - 15s 160ms/step - loss: 0.0029\n",
      "Epoch 24/100\n",
      "95/95 [==============================] - 16s 163ms/step - loss: 0.0030\n",
      "Epoch 25/100\n",
      "95/95 [==============================] - 16s 165ms/step - loss: 0.0027\n",
      "Epoch 26/100\n",
      "95/95 [==============================] - 16s 170ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "95/95 [==============================] - 16s 164ms/step - loss: 0.0028\n",
      "Epoch 28/100\n",
      "95/95 [==============================] - 15s 162ms/step - loss: 0.0028\n",
      "Epoch 29/100\n",
      "95/95 [==============================] - 16s 164ms/step - loss: 0.0028\n",
      "Epoch 30/100\n",
      "95/95 [==============================] - 15s 160ms/step - loss: 0.0029\n",
      "Epoch 31/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0027\n",
      "Epoch 32/100\n",
      "95/95 [==============================] - 16s 173ms/step - loss: 0.0025\n",
      "Epoch 33/100\n",
      "95/95 [==============================] - 17s 180ms/step - loss: 0.0029\n",
      "Epoch 34/100\n",
      "95/95 [==============================] - 19s 202ms/step - loss: 0.0028\n",
      "Epoch 35/100\n",
      "95/95 [==============================] - 18s 190ms/step - loss: 0.0026\n",
      "Epoch 36/100\n",
      "95/95 [==============================] - 17s 184ms/step - loss: 0.0028\n",
      "Epoch 37/100\n",
      "95/95 [==============================] - 17s 183ms/step - loss: 0.0026\n",
      "Epoch 38/100\n",
      "95/95 [==============================] - 16s 165ms/step - loss: 0.0028\n",
      "Epoch 39/100\n",
      "95/95 [==============================] - 17s 174ms/step - loss: 0.0025\n",
      "Epoch 40/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0026\n",
      "Epoch 41/100\n",
      "95/95 [==============================] - 16s 171ms/step - loss: 0.0027\n",
      "Epoch 42/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0024\n",
      "Epoch 43/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0026\n",
      "Epoch 44/100\n",
      "95/95 [==============================] - 16s 169ms/step - loss: 0.0023\n",
      "Epoch 45/100\n",
      "95/95 [==============================] - 16s 169ms/step - loss: 0.0022\n",
      "Epoch 46/100\n",
      "95/95 [==============================] - 16s 171ms/step - loss: 0.0025\n",
      "Epoch 47/100\n",
      "95/95 [==============================] - 17s 175ms/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "95/95 [==============================] - 19s 204ms/step - loss: 0.0023\n",
      "Epoch 49/100\n",
      "95/95 [==============================] - 16s 166ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "95/95 [==============================] - 16s 166ms/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "95/95 [==============================] - 16s 165ms/step - loss: 0.0024\n",
      "Epoch 52/100\n",
      "95/95 [==============================] - 17s 176ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "95/95 [==============================] - 15s 158ms/step - loss: 0.0026\n",
      "Epoch 54/100\n",
      "95/95 [==============================] - 15s 154ms/step - loss: 0.0025\n",
      "Epoch 55/100\n",
      "95/95 [==============================] - 15s 159ms/step - loss: 0.0024\n",
      "Epoch 56/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0025\n",
      "Epoch 57/100\n",
      "95/95 [==============================] - 15s 162ms/step - loss: 0.0027\n",
      "Epoch 58/100\n",
      "95/95 [==============================] - 14s 149ms/step - loss: 0.0022\n",
      "Epoch 59/100\n",
      "95/95 [==============================] - 16s 163ms/step - loss: 0.0023\n",
      "Epoch 60/100\n",
      "95/95 [==============================] - 18s 186ms/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "95/95 [==============================] - 15s 162ms/step - loss: 0.0024\n",
      "Epoch 62/100\n",
      "95/95 [==============================] - 15s 163ms/step - loss: 0.0024\n",
      "Epoch 63/100\n",
      "95/95 [==============================] - 22s 230ms/step - loss: 0.0021\n",
      "Epoch 64/100\n",
      "95/95 [==============================] - 21s 217ms/step - loss: 0.0026\n",
      "Epoch 65/100\n",
      "95/95 [==============================] - 24s 249ms/step - loss: 0.0023\n",
      "Epoch 66/100\n",
      "95/95 [==============================] - 23s 239ms/step - loss: 0.0023\n",
      "Epoch 67/100\n",
      "95/95 [==============================] - 17s 178ms/step - loss: 0.0023\n",
      "Epoch 68/100\n",
      "95/95 [==============================] - 15s 156ms/step - loss: 0.0022\n",
      "Epoch 69/100\n",
      "95/95 [==============================] - 14s 151ms/step - loss: 0.0022\n",
      "Epoch 70/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0022\n",
      "Epoch 71/100\n",
      "95/95 [==============================] - 14s 152ms/step - loss: 0.0023\n",
      "Epoch 72/100\n",
      "95/95 [==============================] - 15s 155ms/step - loss: 0.0023\n",
      "Epoch 73/100\n",
      "95/95 [==============================] - 15s 159ms/step - loss: 0.0022\n",
      "Epoch 74/100\n",
      "95/95 [==============================] - 14s 152ms/step - loss: 0.0021\n",
      "Epoch 75/100\n",
      "95/95 [==============================] - 15s 159ms/step - loss: 0.0021\n",
      "Epoch 76/100\n",
      "95/95 [==============================] - 14s 150ms/step - loss: 0.0023\n",
      "Epoch 77/100\n",
      "95/95 [==============================] - 15s 153ms/step - loss: 0.0022\n",
      "Epoch 78/100\n",
      "95/95 [==============================] - 16s 170ms/step - loss: 0.0023\n",
      "Epoch 79/100\n",
      "95/95 [==============================] - 17s 179ms/step - loss: 0.0023\n",
      "Epoch 80/100\n",
      "95/95 [==============================] - 16s 174ms/step - loss: 0.0024\n",
      "Epoch 81/100\n",
      "95/95 [==============================] - 17s 175ms/step - loss: 0.0020\n",
      "Epoch 82/100\n",
      "95/95 [==============================] - 15s 154ms/step - loss: 0.0021\n",
      "Epoch 83/100\n",
      "95/95 [==============================] - 15s 156ms/step - loss: 0.0021\n",
      "Epoch 84/100\n",
      "95/95 [==============================] - 15s 157ms/step - loss: 0.0022\n",
      "Epoch 85/100\n",
      "95/95 [==============================] - 15s 162ms/step - loss: 0.0022\n",
      "Epoch 86/100\n",
      "95/95 [==============================] - 15s 159ms/step - loss: 0.0020\n",
      "Epoch 87/100\n",
      "95/95 [==============================] - 15s 154ms/step - loss: 0.0022\n",
      "Epoch 88/100\n",
      "95/95 [==============================] - 14s 152ms/step - loss: 0.0021\n",
      "Epoch 89/100\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.0023\n",
      "Epoch 90/100\n",
      "95/95 [==============================] - 13s 138ms/step - loss: 0.0020\n",
      "Epoch 91/100\n",
      "95/95 [==============================] - 13s 138ms/step - loss: 0.0021\n",
      "Epoch 92/100\n",
      "95/95 [==============================] - 13s 140ms/step - loss: 0.0022\n",
      "Epoch 93/100\n",
      "95/95 [==============================] - 16s 167ms/step - loss: 0.0022\n",
      "Epoch 94/100\n",
      "95/95 [==============================] - 15s 154ms/step - loss: 0.0023\n",
      "Epoch 95/100\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.0022\n",
      "Epoch 96/100\n",
      "95/95 [==============================] - 14s 146ms/step - loss: 0.0020\n",
      "Epoch 97/100\n",
      "95/95 [==============================] - 13s 140ms/step - loss: 0.0021\n",
      "Epoch 98/100\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.0022\n",
      "Epoch 99/100\n",
      "95/95 [==============================] - 15s 155ms/step - loss: 0.0020\n",
      "Epoch 100/100\n",
      "95/95 [==============================] - 14s 147ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aff3297460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "model.fit(x, y, epochs=100, batch_size=32)  # Adjust epochs and batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare the test data (you can do this in a similar way)\n",
    "data_test_scaled = scaler.transform(data_test)\n",
    "\n",
    "x_test = []\n",
    "for i in range(time_steps, data_test_scaled.shape[0]):\n",
    "    x_test.append(data_test_scaled[i - time_steps:i])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Make predictions\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Inverse transform the predictions to get actual prices\n",
    "predictions_inverse = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_inverse will contain two columns: [Predicted Open, Predicted Close]\n",
    "predicted_open_price = predictions_inverse[:, 0]\n",
    "predicted_close_price = predictions_inverse[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted prices in the dictionary\n",
    "predictions_dict[stock_symbol] = {\n",
    "    \"Predicted_Open_Prices\": predicted_open_price,\n",
    "    \"Predicted_Close_Prices\": predicted_close_price\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for each stock\n",
    "model.save(f'stock_price_model_GOOG.h5')\n",
    "model.save(f'stock_price_model_AAPL.h5')\n",
    "model.save(f'stock_price_model_MSFT.h5')\n",
    "model.save(f'stock_price_model_AMZN.h5')\n",
    "model.save(f'stock_price_model_TSLA.h5')\n",
    "model.save(f'stock_price_model_NFLX.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models trained and predictions saved.\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "for symbol, prediction in predictions_dict.items():\n",
    "    df = pd.DataFrame({\n",
    "        'Predicted_Open': prediction['Predicted_Open_Prices'],\n",
    "        'Predicted_Close': prediction['Predicted_Close_Prices']\n",
    "    })\n",
    "    df.to_csv(f'predictions_GOOG.csv', index=False)\n",
    "    df.to_csv(f'predictions_AAPL.csv', index=False)\n",
    "    df.to_csv(f'predictions_MSFT.csv', index=False)\n",
    "    df.to_csv(f'predictions_AMZN.csv', index=False)\n",
    "    df.to_csv(f'predictions_TSLA.csv', index=False)\n",
    "    df.to_csv(f'predictions_NFLX.csv', index=False)\n",
    "\n",
    "print(\"All models trained and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
